{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87201262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\abelo\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\abelo\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=0c4bf2e1b7574e4d178c483c9e006cf3c8ede507aa08a698ae57fe83a589ccaa\n",
      "  Stored in directory: c:\\users\\abelo\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1479436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlsxwriter in c:\\users\\abelo\\anaconda3\\lib\\site-packages (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accaf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d13cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3210a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_time = 0.001\n",
    "\n",
    "def collect_links(soup, exclude=set(), link_keyword='/wiki/'):\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        l = link.get('href')\n",
    "        title = link.get('title')\n",
    "        cl = link.get('class')\n",
    "        if l is None or link.text is None or title is None or cl is not None:\n",
    "            continue\n",
    "        if l.startswith(link_keyword) and \".png\" not in l and \".jpg\" not in l and l not in exclude and \"Служебная\" not in l:\n",
    "            links.append(\"https://ru.wikipedia.org\" + l)\n",
    "        else:\n",
    "            pass\n",
    "    return links\n",
    "\n",
    "\n",
    "def wiki_crawler(path, start_url, links_lim=1000):\n",
    "    workbook = xlsxwriter.Workbook(path)\n",
    "    worksheet = workbook.add_worksheet(\"Data\")\n",
    "    for col, item in enumerate([\"prev_page_link\", \"current_page_link\", \"heading\", \"sub_headings\", \"images\"]):\n",
    "            worksheet.write(0, col, item)\n",
    "    \n",
    "    stack = [(start_url, None)]\n",
    "    visited = 0\n",
    "    visited_links = set()\n",
    "\n",
    "    pbar = tqdm(total=links_lim)\n",
    "    while len(stack) > 0 and visited <= links_lim:\n",
    "        url, prev_url = stack.pop(0)\n",
    "        visited_links.add(url) # not to visit again to prevent loops\n",
    "        try:\n",
    "            time.sleep(sleep_time)\n",
    "            resp = requests.get(url)\n",
    "        except e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # maknig a soup of the current website\n",
    "        soup = bs4.BeautifulSoup(resp.content, 'html.parser')\n",
    "        # check if page is valid\n",
    "        heading = soup.find(\"span\", {\"class\": \"mw-page-title-main\"})\n",
    "        if heading is None:\n",
    "            continue\n",
    "            # print(f\"Error with page: {url} ; \\nNo heading!!!\")\n",
    "        else:\n",
    "            heading = heading.text\n",
    "            visited += 1\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # adding all the new links to the stack\n",
    "        links = collect_links(soup, visited_links)\n",
    "        for link in links:\n",
    "            stack.append((link, url)) # new and prev urls\n",
    "            \n",
    "        # collecting the nesessary data\n",
    "        sub_headings = soup.find_all(\"span\", {\"class\": \"mw-headline\"})\n",
    "        sub_headings = \" ; \".join([sub_heading.text for sub_heading in sub_headings])\n",
    "        \n",
    "        images = soup.find_all(\"img\")\n",
    "        images = \" ; \".join([str(image.get('src')).replace(\"https:\", \"\") for image in images if image.get('src') is not None])\n",
    "        \n",
    "        for col, item in enumerate([prev_url, url, heading, sub_headings, images]):\n",
    "            worksheet.write(visited, col, item)\n",
    "        # print(len(stack))\n",
    "    pbar.close()\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1637bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [45:54,  2.18it/s]                            \n"
     ]
    }
   ],
   "source": [
    "start = \"https://ru.wikipedia.org/wiki/%D0%A6%D0%B8%D0%B2%D0%B8%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F\"\n",
    "path = \"./output1.xlsx\"\n",
    "wiki_crawler(path, start, links_lim=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f93d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
